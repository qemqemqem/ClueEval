# CLUE Evaluation Task

This directory contains the CLUE (ClueEval Language Understanding Evaluation) task for the LM Evaluation Harness. This task is designed to evaluate the performance of language models on solving generated mystery stories.

## Overview

The CLUE evaluation task uses the LM Evaluation Harness to assess how well language models can solve mystery stories generated by the ClueEval project. It tests the model's ability to understand complex narratives, identify relevant clues, and make logical deductions to solve the mystery.

## Files

- `clue_questions.jsonl`: Contains the generated mystery stories and their corresponding questions.
- `clue_eval.yaml`: Configuration file for the CLUE task, specifying how to load and process the data.
- `run_evaluation.sh`: Shell script to run the evaluation using the LM Evaluation Harness.

## Setup

1. Install the LM Evaluation Harness:
   ```
   pip install lm-evaluation-harness
   ```
   For more information, visit: https://github.com/EleutherAI/lm-evaluation-harness

2. Ensure you have the necessary API keys if you're using models that require them (e.g., OpenAI API key for GPT models).

## Running the Evaluation

To run the CLUE evaluation:

1. Make the script executable:
   ```
   chmod +x lm_eval/tasks/clue_eval/run_evaluation.sh
   ```

2. Run the script:
   ```
   ./lm_eval/tasks/clue_eval/run_evaluation.sh
   ```

This script uses the `lm_eval` command provided by the LM Evaluation Harness. You can find the exact command and parameters in the `run_evaluation.sh` file.

## Results

The evaluation results will be saved in the directory specified by `results_path` in the `clue_eval.yaml` file. By default, this is set to `lm_eval/tasks/clue_eval/results`.

## Customization

You can customize the evaluation by:
- Modifying the questions in `clue_questions.jsonl`
- Adjusting the task configuration in `clue_eval.yaml`
- Changing the evaluation model or parameters in `run_evaluation.sh`

For more information on the ClueEval project and how the mystery stories are generated, please refer to the main README.md file in the project root directory.
